{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic + Augmentations (Invert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.data import train_test_split, MRISequence\n",
    "from src.model import create_model, compile_model, load_checkpoint\n",
    "from src.model.training import train\n",
    "from src.model.evaluation import plot_training_history, show_simple_metrics, show_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "RANDOM_SEED = 792379571013149\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(tf.version.VERSION)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not copying files since the destination directory already exists\n",
      "initializing train_seq...\n",
      "initializing test_seq...\n",
      "val_seq = test_seq\n",
      "log_dir: ../../tmp\\logs\\20201215-150830\n",
      "Wall time: 5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ROOT_DIR = '../../tmp'\n",
    "DEFAULT_CHECKPOINT_DIRECTORY_LOCAL = os.path.join(ROOT_DIR, 'checkpoints')\n",
    "DEFAULT_BCKP_CHECKPOINT_DIRECTORY_LOCAL = os.path.join(ROOT_DIR, 'bckp-checkpoints')\n",
    "\n",
    "LOG_DIRECTORY = os.path.join(ROOT_DIR, 'logs')\n",
    "CHECKPOINT_DIRECTORY = DEFAULT_CHECKPOINT_DIRECTORY_LOCAL\n",
    "\n",
    "LOG_DIRECTORY_LOCAL = LOG_DIRECTORY\n",
    "CHECKPOINT_DIRECTORY_LOCAL = CHECKPOINT_DIRECTORY\n",
    "\n",
    "DATA_DIR_NAME = 'data-v3'\n",
    "DATA_DIR = os.path.join(ROOT_DIR, DATA_DIR_NAME)\n",
    "\n",
    "saliencies_and_segmentations_v2_path = os.path.join(ROOT_DIR, 'saliencies_and_segmentations_v2')\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_DIRECTORY):\n",
    "    os.mkdir(CHECKPOINT_DIRECTORY)\n",
    "\n",
    "if not os.path.exists(LOG_DIRECTORY):\n",
    "    os.mkdir(LOG_DIRECTORY)\n",
    "\n",
    "val = False\n",
    "    \n",
    "class_names = ['AD', 'CN']\n",
    "\n",
    "# get paths to data\n",
    "train_dir, test_dir, val_dir = train_test_split(\n",
    "    saliencies_and_segmentations_v2_path, \n",
    "    ROOT_DIR, \n",
    "    split=(0.8, 0.15, 0.05), \n",
    "    dirname=DATA_DIR_NAME)\n",
    "\n",
    "# set the batch size for mri seq\n",
    "batch_size = 12\n",
    "input_shape = (104, 128, 104, 1) # (112, 112, 105, 1)\n",
    "resize_img = True\n",
    "crop_img = True\n",
    "\n",
    "# if y is one-hot encoded or just scalar number\n",
    "one_hot = True\n",
    "\n",
    "# class weightss (see analysis notebook)\n",
    "class_weights = {0: 0.8072289156626505, 1: 1.3137254901960784}\n",
    "\n",
    "# description statistics of the dataset\n",
    "desc = {'mean': -3.6344006e-09, 'std': 1.0000092, 'min': -1.4982183, 'max': 10.744175}\n",
    "\n",
    "if 'desc' not in locals():\n",
    "    print('initializing desc...')\n",
    "    desc = get_description(MRISequence(\n",
    "        train_dir,\n",
    "        64,\n",
    "        class_names=class_names,\n",
    "        input_shape=input_shape),\n",
    "        max_samples=None)\n",
    "    print(desc)\n",
    "\n",
    "\n",
    "normalization={ 'type':'normalization', 'desc': desc }\n",
    "# normalization={'type':'standardization', 'desc':desc }\n",
    "\n",
    "augmentations = None\n",
    "augmentations_inplace = True\n",
    "# enable augmentations in mri seq (otherwise it can be enabled in dataset)\n",
    "# augmentations={ 'random_swap_hemispheres': 0.5 }\n",
    "\n",
    "# initialize sequences\n",
    "print('initializing train_seq...')\n",
    "train_seq = MRISequence(\n",
    "    train_dir,\n",
    "    batch_size,\n",
    "    class_names=class_names,\n",
    "    augmentations=augmentations,\n",
    "    augmentations_inplace=augmentations_inplace,\n",
    "    input_shape=input_shape,\n",
    "    resize_img=resize_img,\n",
    "    crop_img=crop_img,\n",
    "    one_hot=one_hot,\n",
    "    class_weights=class_weights,\n",
    "    normalization=normalization)\n",
    "\n",
    "print('initializing test_seq...')\n",
    "test_seq = MRISequence(\n",
    "    test_dir,\n",
    "    batch_size,\n",
    "    class_names=class_names,\n",
    "    input_shape=input_shape,\n",
    "    resize_img=resize_img,\n",
    "    crop_img=crop_img,\n",
    "    one_hot=one_hot,\n",
    "    normalization=normalization)\n",
    "\n",
    "if val:\n",
    "    print('initializing val_seq...')\n",
    "    val_seq = MRISequence(\n",
    "        val_dir,\n",
    "        batch_size,\n",
    "        class_names=class_names,\n",
    "        input_shape=input_shape,\n",
    "        resize_img=resize_img,\n",
    "        crop_img=crop_img,\n",
    "        one_hot=one_hot,\n",
    "        class_weights=class_weights,\n",
    "        normalization=normalization)\n",
    "else:\n",
    "    print('val_seq = test_seq')\n",
    "    val_seq = test_seq\n",
    "    \n",
    "model_key = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "log_dir = os.path.join(LOG_DIRECTORY, model_key)\n",
    "print(f'log_dir: {log_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights\n",
    "# pos / neg\n",
    "initial_bias = np.log([159/243, 243/159])\n",
    "\n",
    "model_config = {\n",
    "    'input_shape': input_shape,\n",
    "    'class_names': class_names,\n",
    "#     'l2_beta': 0.001,\n",
    "#     'l2_beta': 0.0005,\n",
    "#     'l2_beta': None,\n",
    "#     'dropout': 0.10,\n",
    "#     'dropout': None,\n",
    "    'output_bias': initial_bias,\n",
    "#     'output_bias': None,\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "    'batch_norm': False,\n",
    "    'is_complex': False, # a complex layer from the paper, max batch_size is 3\n",
    "}\n",
    "\n",
    "compile_config = {\n",
    "    # default is 0.001\n",
    "#     'learning_rate': 0.000075,\n",
    "    'learning_rate': 0.00025,\n",
    "    'decay_steps': 25,\n",
    "    'decay_rate': 0.96,\n",
    "#     'beta_1': 0.85,\n",
    "    'beta_1': 0.90,\n",
    "#     'beta_2': 0.990,\n",
    "    'beta_2': 0.999,\n",
    "}\n",
    "\n",
    "train_config = {\n",
    "    'model_key': model_key,\n",
    "    'epochs': 200,\n",
    "    'patience': 25,\n",
    "    'tensorboard_update_freq': 'epoch',\n",
    "    'mri_tensorboard_callback': False,\n",
    "    'model_checkpoint_callback': {'monitor': 'val_auc', 'mode': 'max', 'save_best_only': True},\n",
    "    'early_stopping_monitor': {'monitor': 'val_auc', 'mode': 'max'},\n",
    "    'augmentations': {\n",
    "        'invert': (0.5, None),\n",
    "        'rotate': (0, 5), # probability, degrees\n",
    "        'zoom': (0, 0.015),\n",
    "        'shear': (0.2, 2.5), # probability, degrees\n",
    "        'blur': (0, 0.8),\n",
    "        'noise': (0, 0.00025)\n",
    "    },\n",
    "    'batch_size': 10,\n",
    "#     'model_checkpoint_callback': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 104, 128, 104, 32) 896       \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 52, 64, 52, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 52, 64, 52, 64)    55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 17, 21, 17, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 17, 21, 17, 128)   221312    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 4, 5, 4, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10240)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               2621696   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 2,899,778\n",
      "Trainable params: 2,899,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(**model_config)\n",
    "model, *_ = compile_model(model, **compile_config)\n",
    "model.build(input_shape=input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_checkpoint(model, DEFAULT_CHECKPOINT_DIRECTORY_LOCAL, '20201213-182225', 'cp-0002.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model key: 20201215-150830\n",
      "checkpoint dir: ../../tmp\\checkpoints\\20201215-150830\n",
      "log dir: ../../tmp\\logs\\20201215-150830\n",
      "loading ../../tmp\\data-v3\\train_x.npy, ../../tmp\\data-v3\\train_y.npy...\n",
      "loading ../../tmp\\data-v3\\val_x.npy, ../../tmp\\data-v3\\val_y.npy...\n",
      "training...\n",
      "Epoch 1/200\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.7369 - recall: 0.4179 - precision: 0.4179 - auc: 0.4204 - categorical_accuracy: 0.4179\n",
      "Epoch 00001: val_auc improved from -inf to 0.42330, saving model to ../../tmp\\checkpoints\\20201215-150830\\cp-0001.ckpt\n",
      "41/41 [==============================] - 15s 378ms/step - loss: 0.7369 - recall: 0.4179 - precision: 0.4179 - auc: 0.4204 - categorical_accuracy: 0.4179 - val_loss: 0.7069 - val_recall: 0.3800 - val_precision: 0.3800 - val_auc: 0.4233 - val_categorical_accuracy: 0.3800\n",
      "Epoch 2/200\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.7023 - recall: 0.5025 - precision: 0.5025 - auc: 0.4863 - categorical_accuracy: 0.5025\n",
      "Epoch 00002: val_auc improved from 0.42330 to 0.43360, saving model to ../../tmp\\checkpoints\\20201215-150830\\cp-0002.ckpt\n",
      "41/41 [==============================] - 14s 338ms/step - loss: 0.7023 - recall: 0.5025 - precision: 0.5025 - auc: 0.4863 - categorical_accuracy: 0.5025 - val_loss: 0.7141 - val_recall: 0.3800 - val_precision: 0.3800 - val_auc: 0.4336 - val_categorical_accuracy: 0.3800\n",
      "Epoch 3/200\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.7046 - recall: 0.4950 - precision: 0.4950 - auc: 0.4782 - categorical_accuracy: 0.4950\n",
      "Epoch 00003: val_auc did not improve from 0.43360\n",
      "41/41 [==============================] - 14s 335ms/step - loss: 0.7046 - recall: 0.4950 - precision: 0.4950 - auc: 0.4782 - categorical_accuracy: 0.4950 - val_loss: 0.7148 - val_recall: 0.3800 - val_precision: 0.3800 - val_auc: 0.4293 - val_categorical_accuracy: 0.3800\n",
      "Epoch 4/200\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.7041 - recall: 0.4751 - precision: 0.4751 - auc: 0.4769 - categorical_accuracy: 0.4751\n",
      "Epoch 00004: val_auc did not improve from 0.43360\n",
      "41/41 [==============================] - 14s 335ms/step - loss: 0.7041 - recall: 0.4751 - precision: 0.4751 - auc: 0.4769 - categorical_accuracy: 0.4751 - val_loss: 0.7112 - val_recall: 0.3800 - val_precision: 0.3800 - val_auc: 0.4254 - val_categorical_accuracy: 0.3800\n",
      "Epoch 5/200\n",
      "10/41 [======>.......................] - ETA: 8s - loss: 0.6986 - recall: 0.3700 - precision: 0.3700 - auc: 0.3385 - categorical_accuracy: 0.3700"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model, checkpoint_dir, history = train(\n",
    "    model,\n",
    "    train_seq,\n",
    "    test_seq,\n",
    "    CHECKPOINT_DIRECTORY,\n",
    "    LOG_DIRECTORY,\n",
    "    DATA_DIR,\n",
    "    **train_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "show_simple_metrics(model, test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "show_metrics(model, test_seq, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
