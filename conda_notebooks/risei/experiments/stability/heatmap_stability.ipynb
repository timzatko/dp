{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RISEI Stability Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../..')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.data import train_test_split, MRISequence\n",
    "from src.model import create_model, compile_model, load_checkpoint\n",
    "from src.model.evaluation import show_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "plt.rcParams.update({'font.size': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# RANDOM_SEED = 250398\n",
    "# tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(tf.version.VERSION)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ROOT_DIR = '../../../../tmp'\n",
    "DEFAULT_CHECKPOINT_DIRECTORY_LOCAL = os.path.join(ROOT_DIR, 'checkpoints')\n",
    "DEFAULT_BCKP_CHECKPOINT_DIRECTORY_LOCAL = os.path.join(ROOT_DIR, 'bckp-checkpoints')\n",
    "\n",
    "LOG_DIRECTORY = os.path.join(ROOT_DIR, 'logs')\n",
    "CHECKPOINT_DIRECTORY = DEFAULT_CHECKPOINT_DIRECTORY_LOCAL\n",
    "\n",
    "LOG_DIRECTORY_LOCAL = LOG_DIRECTORY\n",
    "CHECKPOINT_DIRECTORY_LOCAL = CHECKPOINT_DIRECTORY\n",
    "\n",
    "DATA_DIR_NAME = 'data-v3'\n",
    "DATA_DIR = os.path.join(ROOT_DIR, DATA_DIR_NAME)\n",
    "\n",
    "saliencies_and_segmentations_v2_path = os.path.join(ROOT_DIR, 'saliencies_and_segmentations_v2')\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_DIRECTORY):\n",
    "    os.mkdir(CHECKPOINT_DIRECTORY)\n",
    "\n",
    "if not os.path.exists(LOG_DIRECTORY):\n",
    "    os.mkdir(LOG_DIRECTORY)\n",
    "\n",
    "val = False\n",
    "\n",
    "class_names = ['AD', 'CN']\n",
    "\n",
    "# get paths to data\n",
    "train_dir, test_dir, val_dir = train_test_split(\n",
    "    saliencies_and_segmentations_v2_path,\n",
    "    ROOT_DIR,\n",
    "    split=(0.8, 0.15, 0.05),\n",
    "    dirname=DATA_DIR_NAME)\n",
    "\n",
    "# set the batch size for mri seq\n",
    "batch_size = 12\n",
    "input_shape = (104, 128, 104, 1) # (112, 112, 105, 1)\n",
    "resize_img = True\n",
    "crop_img = True\n",
    "\n",
    "# if y is one-hot encoded or just scalar number\n",
    "one_hot = True\n",
    "\n",
    "# class weightss (see analysis notebook)\n",
    "class_weights = {0: 0.8072289156626505, 1: 1.3137254901960784}\n",
    "\n",
    "# description statistics of the dataset\n",
    "desc = {'mean': -3.6344006e-09, 'std': 1.0000092, 'min': -1.4982183, 'max': 10.744175}\n",
    "\n",
    "if 'desc' not in locals():\n",
    "    print('initializing desc...')\n",
    "    desc = get_description(MRISequence(\n",
    "        train_dir,\n",
    "        64,\n",
    "        class_names=class_names,\n",
    "        input_shape=input_shape),\n",
    "        max_samples=None)\n",
    "    print(desc)\n",
    "\n",
    "\n",
    "normalization={ 'type':'normalization', 'desc': desc }\n",
    "# normalization={'type':'standardization', 'desc':desc }\n",
    "\n",
    "augmentations = None\n",
    "augmentations_inplace = True\n",
    "# enable augmentations in mri seq (otherwise it can be enabled in dataset)\n",
    "# augmentations={ 'random_swap_hemispheres': 0.5 }\n",
    "\n",
    "# initialize sequences\n",
    "print('initializing train_seq...')\n",
    "train_seq = MRISequence(\n",
    "    train_dir,\n",
    "    batch_size,\n",
    "    class_names=class_names,\n",
    "    augmentations=augmentations,\n",
    "    augmentations_inplace=augmentations_inplace,\n",
    "    input_shape=input_shape,\n",
    "    resize_img=resize_img,\n",
    "    crop_img=crop_img,\n",
    "    one_hot=one_hot,\n",
    "    class_weights=class_weights,\n",
    "    normalization=normalization)\n",
    "\n",
    "print('initializing test_seq...')\n",
    "test_seq = MRISequence(\n",
    "    test_dir,\n",
    "    batch_size,\n",
    "    class_names=class_names,\n",
    "    input_shape=input_shape,\n",
    "    resize_img=resize_img,\n",
    "    crop_img=crop_img,\n",
    "    one_hot=one_hot,\n",
    "    normalization=normalization)\n",
    "\n",
    "if val:\n",
    "    print('initializing val_seq...')\n",
    "    val_seq = MRISequence(\n",
    "        val_dir,\n",
    "        batch_size,\n",
    "        class_names=class_names,\n",
    "        input_shape=input_shape,\n",
    "        resize_img=resize_img,\n",
    "        crop_img=crop_img,\n",
    "        one_hot=one_hot,\n",
    "        class_weights=class_weights,\n",
    "        normalization=normalization)\n",
    "else:\n",
    "    print('val_seq = test_seq')\n",
    "    val_seq = test_seq\n",
    "\n",
    "model_key = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "log_dir = os.path.join(LOG_DIRECTORY, model_key)\n",
    "print(f'log_dir: {log_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights\n",
    "# pos / neg\n",
    "initial_bias = np.log([159/243, 243/159])\n",
    "\n",
    "model_type = '3d_cnn'\n",
    "model_config = {\n",
    "    'input_shape': input_shape,\n",
    "    'class_names': class_names,\n",
    "#     'l2_beta': 0.001,\n",
    "#     'l2_beta': 0.0005,\n",
    "    'l2_beta': None,\n",
    "#     'dropout': 0.05,\n",
    "    'dropout': 0.10,\n",
    "    'output_bias': initial_bias,\n",
    "#     'output_bias': None,\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "    'batch_norm': True,\n",
    "    'is_complex': False, # a complex layer from the paper, max batch_size is 3\n",
    "}\n",
    "\n",
    "compile_config = {\n",
    "    # default is 0.001\n",
    "#     'learning_rate': 0.000075,\n",
    "    'learning_rate': 0.00010,\n",
    "    'decay_steps': 25,\n",
    "    'decay_rate': 0.96,\n",
    "#     'beta_1': 0.85,\n",
    "    'beta_1': 0.90,\n",
    "#     'beta_2': 0.990,\n",
    "    'beta_2': 0.999,\n",
    "}\n",
    "\n",
    "train_config = {\n",
    "    'model_key': model_key,\n",
    "    'epochs': 150,\n",
    "    'patience': 75,\n",
    "    'tensorboard_update_freq': 'epoch',\n",
    "    'mri_tensorboard_callback': False,\n",
    "    'model_checkpoint_callback': {'monitor': 'val_auc', 'mode': 'max', 'save_best_only': True},\n",
    "    'early_stopping_monitor': {'monitor': 'val_auc', 'mode': 'max'},\n",
    "#     'augmentations': False,\n",
    "    'augmentations': {\n",
    "        'invert': (0.5, None),\n",
    "        'rotate': (0.2, 5), # probability, degrees\n",
    "        'zoom': (0., 0.),\n",
    "        'shear': (0.2, 0.5), # probability, degrees\n",
    "        'blur': (0.2, 0.85),\n",
    "        'noise': (0.2, 0.00020)\n",
    "    },\n",
    "    'batch_size': 8,\n",
    "#     'model_checkpoint_callback': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = create_model(model_type, model_config)\n",
    "model, *_ = compile_model(model, **compile_config)\n",
    "model.build(input_shape=input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "load_checkpoint(model, DEFAULT_BCKP_CHECKPOINT_DIRECTORY_LOCAL, '20210308-175324', 'cp-0058.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "show_metrics(model, test_seq, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RISEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.risei import RISEI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "risei_config = {\n",
    "    's': 8, \n",
    "    'p1': 1/3, \n",
    "    'b1': 0,\n",
    "    'b2': 1,\n",
    "    'b2_value': 1,\n",
    "    'in_paint': '2d', \n",
    "    'in_paint_blending': True, \n",
    "    'in_paint_radius': 5,\n",
    "    'in_paint_2d_to_3d': True,\n",
    "    'processes': 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from src.heatmaps.evaluation import get_heatmap\n",
    "\n",
    "risei_batch_size = 480\n",
    "risei = RISEI(input_shape[:-1], debug=False, **risei_config)\n",
    "\n",
    "# we will test this only with one image\n",
    "batch_x, batch_y, *_ = test_seq[0]\n",
    "idx = 0\n",
    "image_x, image_y = batch_x[idx], batch_y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Serializer():\n",
    "    def __init__(self, n, input_shape, in_memory=True):\n",
    "        self.n = n\n",
    "        self.id = int(time.time())\n",
    "        self.input_shape = input_shape\n",
    "        self.save_dir = None\n",
    "        \n",
    "        if in_memory:\n",
    "            self.save_dir = os.path.join(ROOT_DIR, \"risei-stability-cache\")\n",
    "            os.makedirs(self.save_dir, exist_ok=True) # mkdir -p\n",
    "        else:\n",
    "            self.heatmaps = np.zeros((n, *input_shape[:-1]))\n",
    "    \n",
    "    def add_heatmap(self, i, heatmap):\n",
    "        if self.save_dir is not None:\n",
    "            np.save(self.__get_fname(i), heatmap)\n",
    "        else:\n",
    "            self.heatmaps[i] = heatmap\n",
    "            \n",
    "    def get_std(self, processes=8):\n",
    "        if self.save_dir is not None:\n",
    "            pass\n",
    "            return std_heatmaps\n",
    "        else:\n",
    "            return get_std_heatmaps_v2(self.heatmaps) \n",
    "        \n",
    "    def __get_fname(self, i):\n",
    "        return get_fname(self.save_dir, self.id, i)\n",
    "    \n",
    "    \n",
    "def __get_fname(save_dir, _id, i):\n",
    "    return os.path.join(save_dir, f'hmap_{_id}_{i}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heatmaps(n, masks_count, image_x, image_y, tf_reset=5):\n",
    "    serializer = Serializer(n, input_shape, False)\n",
    "\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        try:\n",
    "            print(f\"generating heatmap #{i}...\")\n",
    "            risei = RISEI(input_shape[:-1], debug=False, **risei_config)\n",
    "            heatmap, _, _ = get_heatmap(\n",
    "                image_x,\n",
    "                image_y,\n",
    "                model,\n",
    "                risei,\n",
    "                batch_size=train_config['batch_size'],\n",
    "                masks_count=masks_count,\n",
    "                risei_batch_size=risei_batch_size,\n",
    "                debug=False,\n",
    "                log=True\n",
    "            )\n",
    "\n",
    "            # save the heatmap to all generated heatmaps\n",
    "            serializer.add_heatmap(i, heatmap)\n",
    "            \n",
    "            i = i + 1\n",
    "            \n",
    "            # https://github.com/tensorflow/tensorflow/issues/35010\n",
    "            if i % tf_reset == 0:\n",
    "                tf.keras.backend.clear_session()\n",
    "        except Exception as e:\n",
    "            print(\"there was an error, we will try to generate this heatmap again...\")\n",
    "            print(e)\n",
    "\n",
    "    return serializer\n",
    "\n",
    "def get_std_heatmaps_v1(heatmaps):\n",
    "    std_heatmaps = np.zeros(input_shape[:-1])\n",
    "    \n",
    "    for z in range(heatmaps.shape[1]):\n",
    "        for y in range(heatmaps.shape[2]):\n",
    "            for x in range(heatmaps.shape[3]):\n",
    "                # select the same voxel from all images\n",
    "                voxels = heatmaps[:, z, y, x]\n",
    "                # save the standard deviation for that voxel\n",
    "                # TODO: optimize with axis param\n",
    "                std_heatmaps[z, y, x] = np.std(voxels)\n",
    "                \n",
    "    return std_heatmaps\n",
    "\n",
    "def get_std_heatmaps_v2(heatmaps):\n",
    "    return np.std(heatmaps, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# h1 = generate_heatmaps(10, 12, image_x, image_y)\n",
    "# h2 = generate_heatmaps(10, 12, image_x, image_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(experiments, masks_counts):\n",
    "    for masks_count in masks_counts:\n",
    "        print(f\"generating {experiments}x {masks_count} masks...\")\n",
    "        start = time.time()\n",
    "        # [experiments, z, x, y]\n",
    "        serializer = generate_heatmaps(experiments, masks_count, image_x, image_y)\n",
    "        end = time.time()\n",
    "        # [z, x, y]\n",
    "        std_heatmaps = serializer.get_std()\n",
    "\n",
    "        print(f\"result for {masks_count} masks (t: {datetime.timedelta(seconds=int(end - start))})\")\n",
    "        print(f\"\\tmean std: {np.mean(std_heatmaps)}\")\n",
    "        print(f\"\\tmin std: {np.min(std_heatmaps)}\")\n",
    "        print(f\"\\tmax std: {np.max(std_heatmaps)}\")\n",
    "        print(f\"\\tstd std: {np.std(std_heatmaps)}\")\n",
    "\n",
    "        fPath = os.path.join(ROOT_DIR, \"risei-stability\")\n",
    "        fName = os.path.join(fPath, f\"{int(time.time())}_m{masks_count}.npy\")\n",
    "        os.makedirs(fPath, exist_ok=True) # mkdir -p\n",
    "        print(f\"saving std_heatmaps to {fName} ...\")\n",
    "        np.save(fName, std_heatmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiments = 100\n",
    "# masks_counts = [16, 128, 256, 512]\n",
    "\n",
    "# run(experiments, masks_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracemalloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiments = 100\n",
    "masks_counts = [1024]\n",
    "\n",
    "tracemalloc.start()\n",
    "\n",
    "run(experiments, masks_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pympler import asizeof\n",
    "# a = np.zeros((1024, *input_shape[:-1]))\n",
    "# asizeof.asizeof(a) / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import linecache\n",
    "\n",
    "\n",
    "def display_top(snapshot, key_type='lineno', limit=300):\n",
    "    snapshot = snapshot.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    top_stats = snapshot.statistics(key_type)\n",
    "\n",
    "    print(\"Top %s lines\" % limit)\n",
    "    for index, stat in enumerate(top_stats[:limit], 1):\n",
    "        frame = stat.traceback[0]\n",
    "        # replace \"/path/to/module/file.py\" with \"module/file.py\"\n",
    "        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\n",
    "        print(\"#%s: %s:%s: %.1f MiB\"\n",
    "              % (index, filename, frame.lineno, stat.size / 1024 / 1024))\n",
    "        line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "        if line:\n",
    "            print('    %s' % line)\n",
    "            \n",
    "snapshot = tracemalloc.take_snapshot()\n",
    "\n",
    "display_top(snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guppy import hpy; h=hpy()\n",
    "\n",
    "h.heap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = 100\n",
    "masks_counts = [2048]\n",
    "\n",
    "run(experiments, masks_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = 100\n",
    "masks_counts = [4096]\n",
    "\n",
    "run(experiments, masks_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
