{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../..')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.data import train_test_split, MRISequence\n",
    "from src.model import create_model, compile_model, load_checkpoint\n",
    "from src.model.evaluation import show_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "plt.rcParams.update({'font.size': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# RANDOM_SEED = 250398\n",
    "# tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# print(tf.version.VERSION)\n",
    "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not copying files since the destination directory already exists\n",
      "initializing train_seq...\n",
      "initializing test_seq...\n",
      "val_seq = test_seq\n",
      "log_dir: ../../../../tmp\\logs\\20210403-092508\n",
      "Wall time: 6.11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ROOT_DIR = '../../../../tmp'\n",
    "DEFAULT_CHECKPOINT_DIRECTORY_LOCAL = os.path.join(ROOT_DIR, 'checkpoints')\n",
    "DEFAULT_BCKP_CHECKPOINT_DIRECTORY_LOCAL = os.path.join(ROOT_DIR, 'bckp-checkpoints')\n",
    "\n",
    "LOG_DIRECTORY = os.path.join(ROOT_DIR, 'logs')\n",
    "CHECKPOINT_DIRECTORY = DEFAULT_CHECKPOINT_DIRECTORY_LOCAL\n",
    "\n",
    "LOG_DIRECTORY_LOCAL = LOG_DIRECTORY\n",
    "CHECKPOINT_DIRECTORY_LOCAL = CHECKPOINT_DIRECTORY\n",
    "\n",
    "DATA_DIR_NAME = 'data-v3'\n",
    "DATA_DIR = os.path.join(ROOT_DIR, DATA_DIR_NAME)\n",
    "\n",
    "saliencies_and_segmentations_v2_path = os.path.join(ROOT_DIR, 'saliencies_and_segmentations_v2')\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_DIRECTORY):\n",
    "    os.mkdir(CHECKPOINT_DIRECTORY)\n",
    "\n",
    "if not os.path.exists(LOG_DIRECTORY):\n",
    "    os.mkdir(LOG_DIRECTORY)\n",
    "\n",
    "val = False\n",
    "\n",
    "class_names = ['AD', 'CN']\n",
    "\n",
    "# get paths to data\n",
    "train_dir, test_dir, val_dir = train_test_split(\n",
    "    saliencies_and_segmentations_v2_path,\n",
    "    ROOT_DIR,\n",
    "    split=(0.8, 0.15, 0.05),\n",
    "    dirname=DATA_DIR_NAME)\n",
    "\n",
    "# set the batch size for mri seq\n",
    "batch_size = 12\n",
    "input_shape = (104, 128, 104, 1) # (112, 112, 105, 1)\n",
    "resize_img = True\n",
    "crop_img = True\n",
    "\n",
    "# if y is one-hot encoded or just scalar number\n",
    "one_hot = True\n",
    "\n",
    "# class weightss (see analysis notebook)\n",
    "class_weights = {0: 0.8072289156626505, 1: 1.3137254901960784}\n",
    "\n",
    "# description statistics of the dataset\n",
    "desc = {'mean': -3.6344006e-09, 'std': 1.0000092, 'min': -1.4982183, 'max': 10.744175}\n",
    "\n",
    "if 'desc' not in locals():\n",
    "    print('initializing desc...')\n",
    "    desc = get_description(MRISequence(\n",
    "        train_dir,\n",
    "        64,\n",
    "        class_names=class_names,\n",
    "        input_shape=input_shape),\n",
    "        max_samples=None)\n",
    "    print(desc)\n",
    "\n",
    "\n",
    "normalization={ 'type':'normalization', 'desc': desc }\n",
    "# normalization={'type':'standardization', 'desc':desc }\n",
    "\n",
    "augmentations = None\n",
    "augmentations_inplace = True\n",
    "# enable augmentations in mri seq (otherwise it can be enabled in dataset)\n",
    "# augmentations={ 'random_swap_hemispheres': 0.5 }\n",
    "\n",
    "# initialize sequences\n",
    "print('initializing train_seq...')\n",
    "train_seq = MRISequence(\n",
    "    train_dir,\n",
    "    batch_size,\n",
    "    class_names=class_names,\n",
    "    augmentations=augmentations,\n",
    "    augmentations_inplace=augmentations_inplace,\n",
    "    input_shape=input_shape,\n",
    "    resize_img=resize_img,\n",
    "    crop_img=crop_img,\n",
    "    one_hot=one_hot,\n",
    "    class_weights=class_weights,\n",
    "    normalization=normalization)\n",
    "\n",
    "print('initializing test_seq...')\n",
    "test_seq = MRISequence(\n",
    "    test_dir,\n",
    "    batch_size,\n",
    "    class_names=class_names,\n",
    "    input_shape=input_shape,\n",
    "    resize_img=resize_img,\n",
    "    crop_img=crop_img,\n",
    "    one_hot=one_hot,\n",
    "    normalization=normalization)\n",
    "\n",
    "if val:\n",
    "    print('initializing val_seq...')\n",
    "    val_seq = MRISequence(\n",
    "        val_dir,\n",
    "        batch_size,\n",
    "        class_names=class_names,\n",
    "        input_shape=input_shape,\n",
    "        resize_img=resize_img,\n",
    "        crop_img=crop_img,\n",
    "        one_hot=one_hot,\n",
    "        class_weights=class_weights,\n",
    "        normalization=normalization)\n",
    "else:\n",
    "    print('val_seq = test_seq')\n",
    "    val_seq = test_seq\n",
    "\n",
    "model_key = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "log_dir = os.path.join(LOG_DIRECTORY, model_key)\n",
    "print(f'log_dir: {log_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights\n",
    "# pos / neg\n",
    "initial_bias = np.log([159/243, 243/159])\n",
    "\n",
    "model_type = '3d_cnn'\n",
    "model_config = {\n",
    "    'input_shape': input_shape,\n",
    "    'class_names': class_names,\n",
    "#     'l2_beta': 0.001,\n",
    "#     'l2_beta': 0.0005,\n",
    "    'l2_beta': None,\n",
    "#     'dropout': 0.05,\n",
    "    'dropout': 0.10,\n",
    "    'output_bias': initial_bias,\n",
    "#     'output_bias': None,\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "    'batch_norm': True,\n",
    "    'is_complex': False, # a complex layer from the paper, max batch_size is 3\n",
    "}\n",
    "\n",
    "compile_config = {\n",
    "    # default is 0.001\n",
    "#     'learning_rate': 0.000075,\n",
    "    'learning_rate': 0.00010,\n",
    "    'decay_steps': 25,\n",
    "    'decay_rate': 0.96,\n",
    "#     'beta_1': 0.85,\n",
    "    'beta_1': 0.90,\n",
    "#     'beta_2': 0.990,\n",
    "    'beta_2': 0.999,\n",
    "}\n",
    "\n",
    "train_config = {\n",
    "    'model_key': model_key,\n",
    "    'epochs': 150,\n",
    "    'patience': 75,\n",
    "    'tensorboard_update_freq': 'epoch',\n",
    "    'mri_tensorboard_callback': False,\n",
    "    'model_checkpoint_callback': {'monitor': 'val_auc', 'mode': 'max', 'save_best_only': True},\n",
    "    'early_stopping_monitor': {'monitor': 'val_auc', 'mode': 'max'},\n",
    "#     'augmentations': False,\n",
    "    'augmentations': {\n",
    "        'invert': (0.5, None),\n",
    "        'rotate': (0.2, 5), # probability, degrees\n",
    "        'zoom': (0., 0.),\n",
    "        'shear': (0.2, 0.5), # probability, degrees\n",
    "        'blur': (0.2, 0.85),\n",
    "        'noise': (0.2, 0.00020)\n",
    "    },\n",
    "    'batch_size': 8,\n",
    "#     'model_checkpoint_callback': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_model(model_type, model_config)\n",
    "# model, *_ = compile_model(model, **compile_config)\n",
    "# model.build(input_shape=input_shape)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_checkpoint(model, DEFAULT_BCKP_CHECKPOINT_DIRECTORY_LOCAL, '20210308-175324', 'cp-0058.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# just to test of it is ok to clear the session after loading the weigths\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# show_metrics(model, test_seq, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "\n",
    "# for var in model.trainable_variables:\n",
    "#     data.append(var.value().numpy().transpose())\n",
    "    \n",
    "# data = np.array(data)\n",
    "# np.save('./weights.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.load('./weights.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.mpool1 = nn.MaxPool3d(2, stride=2, padding=0)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        self.mpool2 = nn.MaxPool3d(3, stride=3)\n",
    "        \n",
    "        self.conv3 = nn.Conv3d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(128)\n",
    "        self.mpool3 = nn.MaxPool3d(4, stride=4)\n",
    "        \n",
    "        self.flt = nn.Flatten()\n",
    "        self.dp1 = nn.Dropout(p=0.1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(10240, 256)\n",
    "        self.dp2 = nn.Dropout(p=0.1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.mpool1(x)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.mpool2(x)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.mpool3(x)      \n",
    "        \n",
    "        x = self.flt(x)\n",
    "        x = self.dp1(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        torch.cuda.empty_cache()\n",
    "        x = self.dp2(x)\n",
    "        \n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "(32, 1, 3, 3, 3)\n",
      "torch.Size([32, 1, 3, 3, 3])\n",
      "---\n",
      "(32,)\n",
      "torch.Size([32])\n",
      "---\n",
      "(32,)\n",
      "torch.Size([32])\n",
      "---\n",
      "(32,)\n",
      "torch.Size([32])\n",
      "---\n",
      "(64, 32, 3, 3, 3)\n",
      "torch.Size([64, 32, 3, 3, 3])\n",
      "---\n",
      "(64,)\n",
      "torch.Size([64])\n",
      "---\n",
      "(64,)\n",
      "torch.Size([64])\n",
      "---\n",
      "(64,)\n",
      "torch.Size([64])\n",
      "---\n",
      "(128, 64, 3, 3, 3)\n",
      "torch.Size([128, 64, 3, 3, 3])\n",
      "---\n",
      "(128,)\n",
      "torch.Size([128])\n",
      "---\n",
      "(128,)\n",
      "torch.Size([128])\n",
      "---\n",
      "(128,)\n",
      "torch.Size([128])\n",
      "---\n",
      "(256, 10240)\n",
      "torch.Size([256, 10240])\n",
      "---\n",
      "(256,)\n",
      "torch.Size([256])\n",
      "---\n",
      "(2, 256)\n",
      "torch.Size([2, 256])\n",
      "---\n",
      "(2,)\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (mpool1): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (mpool2): MaxPool3d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (bn3): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (mpool3): MaxPool3d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flt): Flatten(start_dim=1, end_dim=-1)\n",
       "  (dp1): Dropout(p=0.1, inplace=False)\n",
       "  (fc1): Linear(in_features=10240, out_features=256, bias=True)\n",
       "  (dp2): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v data mam ulozene tensorflow vahy po layeroch, takto ich nacitavam do pytorch\n",
    "for value, param in zip(weights, net.parameters()):\n",
    "    print(f\"---\")\n",
    "    print(value.shape) # .astype(np.float64)\n",
    "    tensor = torch.from_numpy(value)\n",
    "    print(param.shape)\n",
    "    param.data = tensor\n",
    "#     print(value[0][0])\n",
    "#     print(param.data.numpy()[0][0])\n",
    "    \n",
    "net.eval().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-984.8062404675438\n",
      "-984.8062404675438\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for p in net.parameters():\n",
    "    s += p.data.cpu().numpy().sum()\n",
    "print(s)\n",
    "\n",
    "s = 0\n",
    "for p in weights:\n",
    "    s += p.sum()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1    [-1, 32, 104, 128, 104]             896\n",
      "       BatchNorm3d-2    [-1, 32, 104, 128, 104]              64\n",
      "         MaxPool3d-3       [-1, 32, 52, 64, 52]               0\n",
      "            Conv3d-4       [-1, 64, 52, 64, 52]          55,360\n",
      "       BatchNorm3d-5       [-1, 64, 52, 64, 52]             128\n",
      "         MaxPool3d-6       [-1, 64, 17, 21, 17]               0\n",
      "            Conv3d-7      [-1, 128, 17, 21, 17]         221,312\n",
      "       BatchNorm3d-8      [-1, 128, 17, 21, 17]             256\n",
      "         MaxPool3d-9         [-1, 128, 4, 5, 4]               0\n",
      "          Flatten-10                [-1, 10240]               0\n",
      "          Dropout-11                [-1, 10240]               0\n",
      "           Linear-12                  [-1, 256]       2,621,696\n",
      "          Dropout-13                  [-1, 256]               0\n",
      "           Linear-14                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 2,900,226\n",
      "Trainable params: 2,900,226\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 5.28\n",
      "Forward/backward pass size (MB): 902.31\n",
      "Params size (MB): 11.06\n",
      "Estimated Total Size (MB): 918.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(net, (1, 104, 128, 104))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = test_seq[2]\n",
    "\n",
    "test_x = torch.from_numpy(np.transpose(batch_x, axes=(0, 4, 3, 2, 1))).float()\n",
    "test_y = torch.from_numpy(batch_y).float()\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29865196 0.70134807]\n",
      " [0.29880002 0.70119995]\n",
      " [0.29912612 0.70087385]\n",
      " [0.29975107 0.7002489 ]\n",
      " [0.29955003 0.70044994]\n",
      " [0.29985633 0.70014364]\n",
      " [0.30012932 0.6998707 ]\n",
      " [0.29898825 0.7010117 ]]\n",
      "[[0.29933766 0.7006624 ]\n",
      " [0.29959735 0.7004027 ]\n",
      " [0.29940754 0.70059246]\n",
      " [0.29936785 0.7006321 ]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "        y = net(inputs.to('cuda'))\n",
    "        y_pred = y.to('cpu').detach().numpy()\n",
    "        print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import select_from_dataset\n",
    "\n",
    "images_x, images_y = select_from_dataset(model, test_seq, max_category=10)\n",
    "\n",
    "print(images_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_explain.core import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = IntegratedGradients()\n",
    "\n",
    "data = (np.array([images_x[0]]), np.array([images_y[0]]))\n",
    "class_index = np.argmax(images_y[0], axis=0)\n",
    "grid = explainer.explain(data, model, class_index=class_index)\n",
    "print(grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(grid[56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_explain.core import GradientsInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = GradientsInputs()\n",
    "\n",
    "data = (np.array([images_x[0]]), np.array([images_y[0]]))\n",
    "class_index = np.argmax(images_y[0], axis=0)\n",
    "grid = explainer.explain(data, model, class_index=class_index)\n",
    "print(grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(grid[56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_explain.core import OcclusionSensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = OcclusionSensitivity(batch_size=8)\n",
    "\n",
    "data = (np.array([images_x[0]]), np.array([images_y[0]]))\n",
    "class_index = np.argmax(images_y[0], axis=0)\n",
    "grid = explainer.explain(data, model, class_index=class_index, patch_size=10)\n",
    "print(grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(grid)\n",
    "\n",
    "# Not working :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Core Module for Grad CAM Algorithm\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "from tf_explain.utils.display import grid_display, heatmap_display\n",
    "from tf_explain.utils.saver import save_rgbZ\n",
    "\n",
    "\n",
    "class GradCAM:\n",
    "\n",
    "    \"\"\"\n",
    "    Perform Grad CAM algorithm for a given input\n",
    "    Paper: [Grad-CAM: Visual Explanations from Deep Networks\n",
    "            via Gradient-based Localization](https://arxiv.org/abs/1610.02391)\n",
    "    \"\"\"\n",
    "\n",
    "    def explain(\n",
    "        self,\n",
    "        validation_data,\n",
    "        model,\n",
    "        class_index,\n",
    "        layer_name=None,\n",
    "        use_guided_grads=True,\n",
    "        colormap=cv2.COLORMAP_VIRIDIS,\n",
    "        image_weight=0.7,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute GradCAM for a specific class index.\n",
    "        Args:\n",
    "            validation_data (Tuple[np.ndarray, Optional[np.ndarray]]): Validation data\n",
    "                to perform the method on. Tuple containing (x, y).\n",
    "            model (tf.keras.Model): tf.keras model to inspect\n",
    "            class_index (int): Index of targeted class\n",
    "            layer_name (str): Targeted layer for GradCAM. If no layer is provided, it is\n",
    "                automatically infered from the model architecture.\n",
    "            colormap (int): OpenCV Colormap to use for heatmap visualization\n",
    "            image_weight (float): An optional `float` value in range [0,1] indicating the weight of\n",
    "                the input image to be overlaying the calculated attribution maps. Defaults to `0.7`.\n",
    "            use_guided_grads (boolean): Whether to use guided grads or raw gradients\n",
    "        Returns:\n",
    "            numpy.ndarray: Grid of all the GradCAM\n",
    "        \"\"\"\n",
    "        images, _ = validation_data\n",
    "\n",
    "        if layer_name is None:\n",
    "            layer_name = self.infer_grad_cam_target_layer(model)\n",
    "\n",
    "        outputs, grads = GradCAM.get_gradients_and_filters(\n",
    "            model, images, layer_name, class_index, use_guided_grads\n",
    "        )\n",
    "\n",
    "        cams = GradCAM.generate_ponderated_output(outputs, grads)\n",
    "\n",
    "        return cams\n",
    "\n",
    "    @staticmethod\n",
    "    def infer_grad_cam_target_layer(model):\n",
    "        \"\"\"\n",
    "        Search for the last convolutional layer to perform Grad CAM, as stated\n",
    "        in the original paper.\n",
    "        Args:\n",
    "            model (tf.keras.Model): tf.keras model to inspect\n",
    "        Returns:\n",
    "            str: Name of the target layer\n",
    "        \"\"\"\n",
    "        for layer in reversed(model.layers):\n",
    "            # Select closest 4D layer to the end of the network.\n",
    "            if len(layer.output_shape) == 5:\n",
    "                return layer.name\n",
    "\n",
    "        raise ValueError(\n",
    "            \"Model does not seem to contain 5D layer. Grad CAM cannot be applied.\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_gradients_and_filters(\n",
    "        model, images, layer_name, class_index, use_guided_grads\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate guided gradients and convolutional outputs with an inference.\n",
    "        Args:\n",
    "            model (tf.keras.Model): tf.keras model to inspect\n",
    "            images (numpy.ndarray): 4D-Tensor with shape (batch_size, H, W, 3)\n",
    "            layer_name (str): Targeted layer for GradCAM\n",
    "            class_index (int): Index of targeted class\n",
    "            use_guided_grads (boolean): Whether to use guided grads or raw gradients\n",
    "        Returns:\n",
    "            Tuple[tf.Tensor, tf.Tensor]: (Target layer outputs, Guided gradients)\n",
    "        \"\"\"\n",
    "        grad_model = tf.keras.models.Model(\n",
    "            [model.inputs], [model.get_layer(layer_name).output, model.output]\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            inputs = tf.cast(images, tf.float32)\n",
    "            conv_outputs, predictions = grad_model(inputs)\n",
    "            loss = predictions[:, class_index]\n",
    "\n",
    "        grads = tape.gradient(loss, conv_outputs)\n",
    "\n",
    "        if use_guided_grads:\n",
    "            grads = (\n",
    "                tf.cast(conv_outputs > 0, \"float32\")\n",
    "                * tf.cast(grads > 0, \"float32\")\n",
    "                * grads\n",
    "            )\n",
    "\n",
    "        return conv_outputs, grads\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_ponderated_output(outputs, grads):\n",
    "        \"\"\"\n",
    "        Apply Grad CAM algorithm scheme.\n",
    "        Inputs are the convolutional outputs (shape WxHxN) and gradients (shape WxHxN).\n",
    "        From there:\n",
    "            - we compute the spatial average of the gradients\n",
    "            - we build a ponderated sum of the convolutional outputs based on those averaged weights\n",
    "        Args:\n",
    "            output (tf.Tensor): Target layer outputs, with shape (batch_size, Hl, Wl, Nf),\n",
    "                where Hl and Wl are the target layer output height and width, and Nf the\n",
    "                number of filters.\n",
    "            grads (tf.Tensor): Guided gradients with shape (batch_size, Hl, Wl, Nf)\n",
    "        Returns:\n",
    "            List[tf.Tensor]: List of ponderated output of shape (batch_size, Hl, Wl, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        maps = [\n",
    "            GradCAM.ponderate_output(output, grad)\n",
    "            for output, grad in zip(outputs, grads)\n",
    "        ]\n",
    "\n",
    "        return maps\n",
    "\n",
    "    @staticmethod\n",
    "    def ponderate_output(output, grad):\n",
    "        \"\"\"\n",
    "        Perform the ponderation of filters output with respect to average of gradients values.\n",
    "        Args:\n",
    "            output (tf.Tensor): Target layer outputs, with shape (Hl, Wl, Nf),\n",
    "                where Hl and Wl are the target layer output height and width, and Nf the\n",
    "                number of filters.\n",
    "            grads (tf.Tensor): Guided gradients with shape (Hl, Wl, Nf)\n",
    "        Returns:\n",
    "            tf.Tensor: Ponderated output of shape (Hl, Wl, 1)\n",
    "        \"\"\"\n",
    "        weights = tf.reduce_mean(grad, axis=(0, 1, 2))\n",
    "\n",
    "        # Perform ponderated sum : w_i * output[:, :, i]\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, output), axis=-1)\n",
    "\n",
    "        return cam\n",
    "\n",
    "    def save(self, grid, output_dir, output_name):\n",
    "        \"\"\"\n",
    "        Save the output to a specific dir.\n",
    "        Args:\n",
    "            grid (numpy.ndarray): Grid of all the heatmaps\n",
    "            output_dir (str): Output directory path\n",
    "            output_name (str): Output name\n",
    "        \"\"\"\n",
    "        save_rgb(grid, output_dir, output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "explainer = GradCAM()\n",
    "\n",
    "idx = 3\n",
    "data = (np.array([images_x[idx]]), np.array([images_y[idx]]))\n",
    "class_index = np.argmax(images_y[idx], axis=0)\n",
    "cams = explainer.explain(data, model, class_index=class_index, layer_name='activation_2')\n",
    "cams = [resize(cam.numpy(), input_shape[:-1]) for cam in cams]\n",
    "print(images_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(cams[0][56])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
